---
title: "GroupI_HW3"
author: "Cortinovis, Cvetinovic, Savarin, Stromieri"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
editor_options:
  markdown:
    wrap: 72
---

# FSDS - Chapter 6

## Ex 6.12

For the UN data file at the book’s website (see Exercise 1.24), construct a multiple regression
model predicting Internet using all the other variables. Use the concept of multicollinearity
to explain why adjusted $R^2$ is not dramatically greater than when GDP is the sole predictor.
Compare the estimated GDP effect in the bivariate model and the multiple regression model and explain why it is so much weaker in the multiple regression model.

```{r, echo=FALSE}
require(MASS)
require(car)
```



**Solution**


```{r}

```

## Ex 6.14

The data set30 Crabs2 at the book’s website comes from a study of factors that affect sperm
traits of male horseshoe crabs. A response variable, SpermTotal, is the log of the total number
of sperm in an ejaculate. It has y = 19.3 and s = 2.0. The two explanatory variables used in
the R output are the horseshoe crab’s carapace width (CW, mean 18.6 cm, standard deviation
3.0 cm), which is a measure of its size, and color (1 = dark, 2 = medium, 3 = light), which is a
measure of adult age, darker ones being older.

(a) Using the results shown, write the prediction equation and interpret the parameter estimates.

(b) Explain the differences in what is tested with the F statistic (i) for the overall model, (ii)
for the factor(Color) effect, (iii) for the interaction term. Interpret each.

**Solution**

(a) 

```{r}

```

(b) 

```{r}

```


## Ex 6.30

When the values of $y$ are multiplied by a constant $c$, from their formulas, show that $s_y$ and $\hat{\beta_1}$ in the bivariate linear model are also then multiplied by $c$. Thus, show that $r = \hat{\beta}_1(s_x/s_y)$ does not depend on the units of measurement.

**Solution**


## Ex 6.42

You can fit the quadratic equation $E(Y) = \beta_0+\beta_11x+\beta_2x^2$ by fitting a multiple regression model with $x1 = x$ and $x2 = x^2$.

(a) Simulate 100 independent observations from the model $Y = 40.0-5.0x+0.5x^2+\epsilon$, where $X$ has a uniform distribution over $[0, 10]$ and $\epsilon \sim N(0, 1)$. Plot the data and fit the quadratic model. Report how the fitted equation compares with the true relationship.

(b) Find the correlation between x and y and explain why it is so weak even though the plot shows a strong relationship with a large $R^2$ value for the quadratic model.

**Solution**

(a) 

```{r}

```

(b) 

```{r}

```


## Ex 6.52

F statistics have alternate expressions in terms of R2 values.

(a) Show that for testing $H_o:\beta1 = \dots = \beta_p = 0$, $$F=\frac{(TSS-SSE)/p}{SSE/[n-(p+1)]} = \frac{R^2/p}{(1-R^2)/[n-(p+1)]}.$$

Explain why larger values of $R^2$ yield larger values of $F$.

(b) Show that for comparing nested linear models, $$F=\frac{(SSE_0-SSE_1)/(p_1-p_0)}{SSE_1/[n-(p_1+1)]} = \frac{(R_1^2-R_0^2)/(p_1-p_0)}{(1-R_1^2)/[n-(p_1+1)]}$$

**Solution**

(a)

```{r}

```

(b)

```{r}

```

# FSDS - Chapter 7

## Ex 7.4

Analogously to the previous exercise, randomly sample 30 X observations from a uniform in the interval (-4,4) and conditional on $X = x$, 30 normal observations with $E(Y) = 3.5x^3-20x^2+0.5x+20$ and $\sigma = 30$. Fit polynomial normal GLMs of lower and higher order than that of the true relationship. Which model would you suggest? Repeat the same task for $E(Y)=0.5x^3-20x^2+0.5x+20$ (same $\sigma$) several times. What do you observe? Which model would you suggest now?


**Solution**

```{r}

```


## Ex 7.20

In the Crabs data file introduced in Section 7.4.2, the variable y indicates whether a female horseshoe crab has at least one satellite (1 = yes, 0 = no).

(a)
Fit a main-effects logistic model using weight and categorical color as explanatory variables. Conduct a significance test for the color effect, and construct a 95% confidence interval for the weight effect.

(b)
Fit the model that permits interaction between color as a factor and weight in their effects, showing the estimated effect of weight for each color. Test whether this model provides a significantly better fit.

(c) 
Use AIC to determine which models seem most sensible among the models with (i) interaction, (ii) main effects, (iii) weight as the sole predictor, (iv) color as the sole predictor, and (v) the null model.

**Solution**

(a) We first load the dataset and perform some light data exploration
```{r}

# Loading the dataset
Crabs <- read.table("http://stat4ds.rwth-aachen.de/data/Crabs.dat", header=TRUE) 
head(Crabs)
attach(Crabs)

unique(color)
```
Given that there're 4 different colors for the `color` feature we fit the following logistic model:
\[
    logit(E(Y)) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4
\]
$X_1$ refers to the `weight` while $X_2,X_3$ and $X_4$ are all dummy variables for the categorical variable `color`.
```{r}
# Fitting a logistic model with weight and categorial color
fit <- glm(y ~  weight + factor(color), family = binomial(link = logit))
summary(fit)
```

Using `anova()` function we test for the color effect:
```{r}
anova(fit, test = "Chisq")
```
Given that we obtained a rather low drop in the residual deviance when we added the `color` variable, and that the p.value is greater than 0.05, we can suspect that the `color` to be marginally impactful for modelling whether a female horseshoe crab has at least one satellite when paired with the main effect of `weight`.

We can compute the $95\%$ confidence interval for the weight effect using Wald test, remembering that we must then compute the exponential for this value since the effect is measured in odds of $E(Y)$.
```{r}
ConfInterval <- fit$coefficients[2] + c(-1,1)* summary(fit)$coefficients[2,2] * qnorm(0.975)
exp(ConfInterval)
```

(b) For our second model we fit:
\[
   \begin{aligned} logit(E(Y)) & = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_1X_2 + \beta_6X_1X_3 + \beta_7X_1X_4 \\
    & = \beta_0 + \beta_1X_1 + X_2(\beta_2 + X_1\beta_5) + X_3(\beta_3 + X_1\beta_6) + X_4(\beta_4 + X_1\beta_4)
    \end{aligned}
\]
```{r}
fit2 <- glm(y ~  weight + factor(color) + weight:factor(color), family = binomial(link = logit))
summary(fit2)
```
Again we can use the `anova()` function to test whether we were able to gain more information on the process
```{r}
anova(fit2, test = "Chisq")
```
We can again see a rather high p.value and a small drop in the residual deviance, leading us to believe that the additional variables are not needed. This result doesn't surprise us given that this model has an AIC score of $197.66$ and the previous one is, for model checking purposes, identical at $198.54$.

(c) Computing the AIC in the order given by the exercise we see:

```{r}
scores <- matrix( c(fit2$aic, fit$aic, AIC(glm(y~ weight, family = binomial())),
                    AIC(glm(y~factor(color), family = binomial())), AIC(glm(y~ 1, family = binomial()))), nrow = 5, ncol = 1, dimnames = list(c("With interaction", "With main effects", "Weight as sole predictor", "Color as sole predictor", "Null model"), c("AIC")))


scores
```
Given that for the first three models the AIC scores is similar, we can follow Occam's razor theory and choose the model with the least amount of explanatory variables between those: $logit(E(Y)) = \beta_0 + \beta_1X_1$ with $X_1$ being the `weight` with an AIC of 199.7371.

## Ex 7.26

A headline in The Gainesville Sun (Feb. 17, 2014) proclaimed a worrisome spike in shark attacks in the previous two years. The reported total number of shark attacks in Florida per year from 2001 to 2013 were 33, 29, 29, 12, 17, 21, 31, 28, 19, 14, 11, 26, 23. Are these counts consistent with a null Poisson model? Explain, and compare aspects of the Poisson model and negative binomial model fits.

**Solution**
```{r}
SharksAttacks <- c(33, 29, 29, 12, 17, 21, 31, 28, 19, 14, 11, 26, 23)
Attacks <- data.frame(SharksAttacks)

# Null poisson model
NullPo <- glm(SharksAttacks ~ 1, data = Attacks, family = poisson) 
summary(NullPo)
```
We now compute the mean and the variance for the `SharksAttack` data:
```{r}
mean(SharksAttacks)
```
```{r}
var(SharksAttacks)
```

The values $\overline{y} = 22.53846$ and  $s^2 = 55.76923$ suggest overdispersion (a poisson model implies that $var(Y) = E(Y)= \mu$), therefore we can believe the null poisson model not to be adequate. Now we test a negative binomial null model
```{r}
# Null negative binomial model
NullNegBin <- glm.nb(SharksAttacks ~ 1, link = log, data = Attacks)
summary(NullNegBin)
```
The AIC for the negative binomial is only slightly better being about 5 units lower, the estimated dispersion parameter is $\frac{1}{k} = 1/15.5 \approx 0.064$ but with a standard error of $10.5$ that we can impute to a very small sample size. Since the dispersion parameter is small we can conclude that the negative binomial variance $\mu + \frac{\mu^2}{k}$ is  similar to the poisson variance, therefore there isn't much difference between the two models fit.


# DAAG - Chapter 8

## Ex 6

As in the previous exercise, the function poissonsim() allows for experimentation with Poisson regression. In particular, poissonsim() can be used to simulate Poisson responses with log-rates equal to $a + bx$, where a and b are fixed values by default.

(a) Simulate 100 Poisson responses using the model $$log\lambda=2-4x$$
for $x=0,0.01,0.02,...,1.0$. Fit a Poisson regression model to these data, and compare the estimated coefficients with the true coefficients. How well does the estimated model predict future observations?

(b) Simulate 100 Poisson responses using the model $$log\lambda=2-bx$$
where b is normally distributed with mean 4 and standard deviation 5. [Use the argument slope.sd=5 in the poissonsim() function.] How do the results using the poisson and quasipoisson families differ?



**Solution**

(a)
```{r}

```

(b)
```{r}

```
